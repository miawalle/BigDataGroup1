
---
title: "Habits"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
---


```{r setup, echo = F}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, results = 'asis', out.height = "\\textheight",  out.width = "\\textwidth")

```


\pagebreak


```{r, cache = FALSE}

library(dplyr)
library(magrittr)
library(tidyr)
library(ggplot2)
library(Hmisc)
library(purrr)
library(caret)
library(ranger) 
library(xgboost)
library(kableExtra) # just to make the output nicer
library(mice)
library(stargazer)
library(xtable)
library(pROC) # think we use yardstick instead, nicer plots
library(knitr)
library(grid)
library(yardstick)
library(SciViews)
library(gridExtra)

options(xtable.comment=FALSE)

theme_set(theme_bw()) 

df <- read.csv("pd_data_v2.csv", sep = ";", header = TRUE)

df_corp <- df

```

# Task 1: Predicion modelling

## Part 1: Inspecting, cleaning and imputing the data set 

In this section, we will train and rund three different prediction models based on a data set containing financial records of a list of companies. In total, there are 24 variables in the data set and xxx observations. Prior to training our prediction models, we load and examine the data set.

The first thing we notice is that some variables are classified as numeric when they should be factor variables, and vice versa. We reclassify these variables. 


Table xx displays summary statistics of our data. As shown in the table, the value xx is present in many of the observations. Most likely, these values represent measurement errors or missing variables, and we therefore choose to replace these values with NAs. After completing this step, there are still some quite extreme outliers remaining in the data. Our assessment is that these values do not represent real observations, as they deviate from typical values for the financial key figures represented in the variables in question. Applying a threshold of 2,5 percent at each end of the variables' distribution, we replace values exceeding this threshold with NAs.  

```{r, cache= TRUE}


# Summary statistics 

caption_at_bottom <- function(expr) {
  x <- capture.output(expr)
  cap <- grep("\\\\caption", x)
  lab <- grep("\\\\label", x)
  last <- grep("\\\\end\\{table", x)
  cat(
    paste(
      c(x[-last], x[cap], x[lab], x[last])[-c(cap, lab)]
    , collapse = "\n")
  , "\n")
}

caption_at_bottom(stargazer(df_corp, summary.stat = c("min","max"),header = FALSE, title  = "Summary statistics"))

# As described in the text, outliers are replaced with NA's. See chunch no. xx. 

```

We also notice that the variable default, which will be the dependent variable in our prediction models, is imbalanced. That is, observations of defaulting companies are underrepresented, which will make it difficult to predict defaults. The underrepresentation of defaults is illustrated in figure \ref{fig:imbalance}. we will handle this imbalance when we implement the prediction models in section xx.

```{r hist, cache=FALSE, fig.cap="\\label{fig:hist}Histogram of the distribution"}

# Distribution of defaults and not defaults

df_corp$default <- as.factor(df_corp$default) 

df_corp %>% 
  ggplot(aes(x = default, fill = default)) +
  geom_bar() +
  ggtitle("Distribution of the none-default/default") +
  theme(plot.title = element_text(hjust = 0.5))

```


```{r, cache= TRUE}

# Mia: Should we consider removing some of these lines? 

# Look at the distribution and recategorize the factor classes

# Adverse_audit_opinion



# xtable(table(df_corp$default, df_corp$adverse_audit_opinion))

df_corp$adverse_audit_opinion <- 
  ifelse(df_corp$adverse_audit_opinion == 0, yes = 0, no = 1)

# xtable(table(df_corp$default, df_corp$adverse_audit_opinion)) # If you got any Adverse audit opinons then its show as 1

# Industry

# xtable(table(df_corp$default, df_corp$industry)) # As we don't know anything about the type of industry we let it be

# Payment_reminders

# xtable(table(df_corp$default, df_corp$payment_reminders)) # Ok

```


```{r}

# Change classes

df_corp$adverse_audit_opinion <- as.factor(df_corp$adverse_audit_opinion) 
df_corp$industry <- as.factor(df_corp$industry)
df_corp$payment_reminders <- as.factor(df_corp$payment_reminders) 
df_corp$equity <- as.numeric(df_corp$equity)
df_corp$total_assets <- as.numeric(df_corp$total_assets)
df_corp$revenue <- as.numeric(df_corp$revenue)
df_corp$age_of_company <- as.numeric(df_corp$age_of_company)

```

```{r}

# Replacing outliers with NA


# First: Take a closer look at the distribution of the values we belive are errors

x <- df_corp %>% 
  select_if(is.numeric)

placeholder <- matrix(ncol=ncol(x), nrow=1)
colnames(placeholder) <- names(x)


for(i in 1:ncol(x)){
  placeholder[,i] <- ifelse(x[,i] > 100000000000 , yes = 1 , 
                            no = ifelse(x[,i] < -100000000000, yes = 1, no = 0)) %>% 
    sum()
}



# as.data.frame(placeholder/nrow(df_corp)) %>%
#  gather() %>% 
#  xtable()

# Do we want to mention this fraction in the text?

# Change the obvious errors to NA

df_corp[df_corp < -1000000000000000] <- NA 
df_corp[df_corp >  1000000000000000] <- NA


```


```{r pre_boxplot, cache = TRUE, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:pre_boxplot}Before cleaning"}

# We take a new look after replacing the errors with NA

df_corp %>% 
  select(which(sapply(.,class)=="numeric"),default) %>%   
  gather(metric, value, -default) %>% 
  ggplot(aes(x= default, y=value, fill = default))+
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

# These boxplots can be viewed in the appendix.

```


```{r, cache= TRUE}

################## Dealing with the outliers ######################


# Function for replacing outliers with NA's

remove_outliers_na <- function(x) { # To be used on other variables
  qnt <- as.vector(quantile(x, probs=c(0.025, 0.975), na.rm = TRUE))
  y <- x
  y[x < qnt[1]] <- NA
  y[x > qnt[2]] <- NA
  y
}

# Strategy: Replace observarions exceeding treshold values of 0.025 (lower bound) 0.975 (upper bound) with NA. This is done for all numeric variables except from age_of_company, gross_operating_inc_perc, paid_debt_collection and unpaid_debt collection.

#           paid and unpaid debt collection will be categorized.

df_corp$profit_margin <- remove_outliers_na(df_corp$profit_margin)
df_corp$operating_margin <- remove_outliers_na(df_corp$operating_margin)
df_corp$EBITDA_margin <- remove_outliers_na(df_corp$EBITDA_margin)
df_corp$interest_coverage_ratio <- remove_outliers_na(df_corp$interest_coverage_ratio)
df_corp$cost_of_debt <- remove_outliers_na(df_corp$cost_of_debt)
df_corp$interest_bearing_debt <- remove_outliers_na(df_corp$interest_bearing_debt)
df_corp$revenue_stability <- remove_outliers_na(df_corp$revenue_stability)
df_corp$equity_ratio <- remove_outliers_na(df_corp$equity_ratio)
df_corp$equity_ratio_stability <- remove_outliers_na(df_corp$equity_ratio_stability)
df_corp$liquidity_ratio_1 <- remove_outliers_na(df_corp$liquidity_ratio_1)
df_corp$liquidity_ratio_2 <- remove_outliers_na(df_corp$liquidity_ratio_2)
df_corp$liquidity_ratio_3 <- remove_outliers_na(df_corp$liquidity_ratio_3)
df_corp$equity <- remove_outliers_na(df_corp$equity)
df_corp$total_assets <- remove_outliers_na(df_corp$total_assets)
df_corp$revenue <- remove_outliers_na(df_corp$revenue)
df_corp$amount_unpaid_debt <- remove_outliers_na(df_corp$amount_unpaid_debt)

df_corp$default <- recode_factor(
  df_corp$default,
  `0` = "No",
  `1` = "Yes")

df_corp <- df_corp %>% 
  select(-equity_ratio_stability) # Removed due to collinearity and if look at the earlier plots it make sense.

```
After cleaning the data set, we are left with xx missing values. As these account for a substantial fraction of the total number of observations in the data set, we choose to impute these using the MICE package. 

- PMM imputation: Explain this. 
- Why is this better than replacing NA with mean or meadian. 

An illustration of the imputation added to the appendix.

- Write some more about the imputation method.

```{r,fig.asp=1, fig.width = 14, eval=FALSE}

# Imputing NAs

temp_imputed1 <- mice(df_corp, m=2,maxit=3,meth='mean',seed=500)

temp_imputed2 <- mice(df_corp, m=2,maxit=3,meth='pmm',seed=500)

saveRDS(temp_imputed1, file = "imputeMEAN.Rdata")

saveRDS(temp_imputed2, file = "impute.Rdata") 

densityplot(temp_imputed1, drop.unused.levels = TRUE) # Save plots again

densityplot(temp_imputed2, drop.unused.levels = TRUE) # Save plots again
?densityplot

# Running the imputations and generating density plots of the imputed values is higly time consuming. Therefore, we have saved these files and will upload them in chunk xx. 

# See appendix for plots. 
```

```{r, include = FALSE}

# Uploading imputed values

temp_imputed1 <- readRDS(file = "imputeMEAN.Rdata")
temp_imputed2 <- readRDS(file = "impute.Rdata")

# We complete the impution and check the result

complete_imputed <- complete(temp_imputed2, 1)

Orginal <- dim(df_corp)
Imputed <- dim(complete_imputed)

xtable(rbind(Orginal, Imputed))

df_cleaned <- complete_imputed

```




A final step is to make sure all variables are classified correctly. We believe the variables unpaid debt collection and paid debt collection are better measured as categorized variables. This is due to their distribution, that includes many extreme values despite our previous cleaning. These are therefore coded as dummies.

```{r, include = FALSE}

# Generating dummies for paid and unpaid debt collection

df_cleaned$unpaid_debt_collection <- ifelse(df_cleaned$unpaid_debt_collection > 0, yes = 1, no = 0) # 1 = Companies that have unpaid debt collection

df_cleaned$paid_debt_collection <- ifelse(df_cleaned$paid_debt_collection > 0, yes = 1, no = 0) # 1 = Companies that have previously had debt collection

df_cleaned$paid_debt_collection <- as.factor(df_cleaned$paid_debt_collection)
df_cleaned$unpaid_debt_collection <- as.factor(df_cleaned$unpaid_debt_collection)

#table1 <- table(df_cleaned$default, df_cleaned$unpaid_debt_collection)
#table2 <- table(df_cleaned$default, df_cleaned$paid_debt_collection)

#xtable(table1)
#xtable(table2)

```
After cleaning, imputing and reclassifying our variables, we are left with data that is well suited for prediction modelling. Figure xx shows the distribution of the variables in our new dataset that will be used moving forward.

```{r Density_post_cleaning, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:Density_post_cleaning}Density plot after cleaning"}

# Density plot after cleaning


df_cleaned %>% 
  select_if(is.numeric) %>% 
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric))+
  geom_density(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

```

```{r Boxplot_post_cleaning, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:Boxplot_post_cleaning}Boxplot after cleaning", include=FALSE}

# Boxplot after cleaning

df_cleaned %>% 
  select(which(sapply(.,class)=="numeric"),default) %>%   
  gather(metric, value, -default) %>% 
  ggplot(aes(x= default, y=value, fill = default))+
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

#This can be viewed in the appendix.

```

##Prediction modelling

In this section we will train and rund three different prediction models; a Logistic Regression Model, a Random Forest and a Xgboost Model. 

### Preparations


Prior to training our models, need to consider potential correlation between the variables in our dataset. This is done by running a logistic regression and letting R calculate the VIF for the different different independent variables. The variables
 \textit{operating margin} and \textit{xxx} have VIF values of xxx and xxx, respectively, and we therefore choose to leave these out when we train our logistic regression model in section xx.  
 

```{r}

# For all models we will remove the equity, total_assets, revenue and amount_unpaid_dept because we want to work with relative values.

df_reduced <- df_cleaned %>% 
  select(-equity, -total_assets, -revenue, -amount_unpaid_debt)

``` 

```{r, include = FALSE, eval= FALSE}

# Last thing we do before we split the data and start modelling is checking for mulitcollinearity.
# To do this we will be using vic function in the car package.

library(car)

mc_vif_1 <- vif(glm(default ~ ., data = df_reduced, family = "binomial"))

mc_vif_1

# We see that for the glm we should remove operating margin

mc_vif_2 <- vif(glm(default ~ .-operating_margin, data = df_reduced, family = "binomial"))

mc_vif_2 # By checking again all looks okay

# Stepwise

# stepwise <- step(glm(default ~ .-operating_margin, data = df_reduced, family = "binomial"), direction="both")

# Removed after stepwise

mc_vif_3 <- vif(glm(default ~ .-operating_margin - paid_debt_collection - liquidity_ratio_1 - profit_margin, data = df_reduced, family = "binomial"))

mc_vif_3
```

```{r}

# Split data in to train/test

set.seed(1)

index <- createDataPartition(df_reduced$default, p = 0.7, list = FALSE)
train_data <- df_reduced[index, ]
test_data <- df_reduced[-index, ]

# Check

nrow(train_data) + nrow(test_data) == nrow(df_reduced)

```

```{r, include = FALSE}

`Train defaults`<- summary(train_data$default)[2]/nrow(train_data)

`Test defaults`<- summary(test_data$default)[2]/nrow(test_data)

xtable(rbind(`Train defaults`,`Test defaults`))

```

```{r}

############# Making some models ###################

ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats =10, 
                     verboseIter = FALSE,
                     sampling = "smote",
                     classProbs=TRUE)

```

### Logistic Regression Model

Our first prediction model is a logistic regression model. 
- Using cross validation: ten fold, 
- Oversampling method (smote) implemented here
- Variable selection: choosing varibles, and why. Not relative values etc etc


Summary statistics from the logistic regression model are presented in table xxxx. 
- comments: which variables are significant, and do they make economically sense

Figure xxx gives and overview of the variable importance.  
- Comments: 

A confusion matrix from the glm model is presented in table xx. 
- Comments: 

Figure xx shows the AUC for the glm model. 
- Comments: 

\pagebreak

```{r}

# After we have runned the model we see that industry and gross have some insignificant values,
# we try to remove gross and ndustry one by one and together. We got lowest AIC by only removing
# gross_operating_inc_perc. 

set.seed(1)

model_glm <- train(default ~ .- operating_margin 
                              - paid_debt_collection 
                              - liquidity_ratio_1 
                              - profit_margin
                              - gross_operating_inc_perc,
                   data = train_data,
                   method = "glm",
                   trControl = ctrl)

# To mangage to run the latter codes due to removal of variables in the regression.



```



```{r var_imp_GLM, fig.cap="\\label{fig:var_imp_GLM}Variable importance"}

summary_glm <- xtable(summary(model_glm), caption = "Summary glm model")

print(summary_glm, scalebox=1, caption.placement = "bottom", label = "tab:sumtable")

plot(varImp(model_glm), main = "Variables Importance GLM")



glm_pred <- data.frame(actual = test_data$default,
                       predict(model_glm, newdata = test_data, type = "prob"))

glm_pred$predict <- ifelse(glm_pred$Yes > 0.5, "Yes", "No")
glm_pred$predict <- as.factor(glm_pred$predict)

cm_glm <-confusionMatrix(glm_pred$predict, test_data$default)

confusion_glm <- xtable(as.matrix(cm_glm$table), caption = "Confusion matrix GLM")

print(confusion_glm, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_glm")



perf_glm <- as.matrix(cm_glm$byClass)
colnames(perf_glm) <- c("Values")

performance_glm <- xtable(perf_glm, caption = "Performance indicators GLM")

print(performance_glm, scalebox = 1, caption.placement = "bottom", label = "tab:performance_glm")

```


```{r ROC_glm, fig.cap="\\label{fig:ROC_glm}ROC plot for glm logistic"}

# ROC curve glm

glm_predictions <- predict(model_glm, test_data, type="prob")

test_glm <- test_data %>%
  select(default) %>% 
  mutate(glm_prob_predictions = glm_predictions$Yes)

glm_auc <- test_glm %>% 
              roc_auc(default, glm_prob_predictions)

glm_auc <- paste(round(glm_auc$.estimate, 4), sep = "")

test_glm %>%
  roc_curve(default, glm_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = glm_auc) +
  coord_equal() +
  ggtitle("ROC curve GLM (Logistic)") +
  theme(plot.title = element_text(hjust = 0.5))
 

```

## Random forest

Our second predicion model is a random forest model.
Comments:
- We have implemented cross validation
- Fine tuning?
- Oversampling technique is also implemented here. 


The model's confusion matrics is presented in figure xx, and the ROC curve is presented in figure xx. 
- Discuss performance 


```{r, eval= FALSE, fig.asp=1, fig.width = 14}

############################ Model 2: Random Forest ######################################

tunegrid <- expand.grid(.mtry= 4,
                        .splitrule = "gini",
                        .min.node.size = 10) # tuning with mtry = 1:5, min.node.size = 10,20

set.seed(1)

model_rf <- caret::train(default ~ .,
                         data = train_data,
                         method = "ranger",
                         trControl = ctrl,
                         tuneGrid = tunegrid,
                         num.trees = 100,
                         importance = "permutation")

# saveRDS(model_rf, file = "rf.Rdata")

```

\pagebreak


```{r var_imp_rf, fig.cap="\\label{fig:var_imp_rf}Variables importance Random Forest"}

model_rf <- readRDS("rf.Rdata")

rf_pred <- data.frame(actual = test_data$default,
                       predict(model_rf, newdata = test_data, type = "prob"))

rf_pred$predict <- ifelse(rf_pred$Yes > 0.5, "Yes", "No")
rf_pred$predict <- as.factor(rf_pred$predict)

plot(varImp(model_rf),main = "Variables Importance Random Forest")

cm_rf <- confusionMatrix(rf_pred$predict, test_data$default)

confusion_rf <- xtable(as.matrix(cm_rf$table), caption = "Confusion matrix Random Forest")

print(confusion_rf, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_rf")

perf_rf <- as.matrix(cm_rf$byClass)
colnames(perf_rf) <- c("Values")

performance_rf <- xtable(perf_rf, caption = "Performance indicators Random Forest")

print(performance_rf, scalebox = 1, caption.placement= "bottom", label = "tab:performance_rf")

```


Comments:
About the confustion matrix
How well the model performs relative to glm
Accuracy, specificity and sensitivity etc.


```{r ROC_rf, fig.cap="\\label{fig:ROC_rf}ROC plot for Random Forest"}

# ROC curve rf

rf_predictions <- predict(model_rf, test_data, type = "prob")

test_rf <- test_data %>%
  select(default) %>% 
  mutate(rf_prob_predictions = rf_predictions$Yes)

rf_auc <- test_rf %>% 
              roc_auc(default, rf_prob_predictions)

rf_auc <- paste(round(rf_auc$.estimate, 4), sep = "")

test_rf %>%
  roc_curve(default, rf_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = rf_auc) +
  coord_equal() +
  ggtitle("ROC curve Random Forest (Ranger)") +
  theme(plot.title = element_text(hjust = 0.5))

```

\pagebreak

# xgBoost

```{r, eval=FALSE, fig.asp=1, fig.width = 14, eval = TRUE}
#Model 3: Xgboost

xgb_grid <- expand.grid(nrounds = 300,
                        max_depth = 7, 
                        min_child_weight = 1,
                        subsample = 1,
                        gamma = .1,
                        colsample_bytree = 0.8,
                        eta = .1)

set.seed(1)

model_xgb <- caret::train(default ~ .,
                          data = train_data,
                          method = "xgbTree",
                          tuneGrid =xgb_grid,
                          trControl = ctrl)

# Check preProcess - is this necessary, is it right, or is it wrong??

# saveRDS(model_xgb, file = "xgb.Rdata")

```

```{r var_imp_xgb, Cache = FALSE, fig.cap="\\label{fig:var_imp_xgb}Variable importance Xgboost"}

model_xgb <- readRDS("xgb.Rdata")

plot(varImp(model_xgb),main = "Variables Importance Xgboost")

xgb_pred <- data.frame(actual = test_data$default,
                      predict(model_xgb, newdata = test_data, type = "prob"))

xgb_pred$predict <- ifelse(xgb_pred$Yes > 0.5, "Yes", "No")
xgb_pred$predict <- as.factor(xgb_pred$predict)

cm_xgb <- confusionMatrix(xgb_pred$predict, xgb_pred$actual)

confusion_xgb <- xtable(as.matrix(cm_xgb$table), caption = "Confusion matrix xgboost")

print(confusion_xgb, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_xgb")

perf_xgb <- as.matrix(cm_xgb$byClass)
colnames(perf_xgb) <- c("Values")

performance_xgb <- xtable(perf_xgb, caption = "Performance indicators xgboost")

print(performance_xgb, scalebox = 1, caption.placement= "bottom", label = "tab:performance_xgb")

```

```{r ROC_xgb, Cache = FALSE, fig.cap="\\label{fig:ROC_xgb}ROC plot for Xgboost"}

# ROC curve xgb

xgb_predictions <- predict(model_xgb, test_data, type = "prob")

test_xgb <- test_data %>%
  select(default) %>% 
  mutate(xgb_prob_predictions = xgb_predictions$Yes)

xgb_auc <- test_xgb %>% 
              roc_auc(default, xgb_prob_predictions)

xgb_auc <- paste(round(xgb_auc$.estimate, 4), sep = "")

test_xgb %>%
  roc_curve(default, xgb_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = xgb_auc) +
  coord_equal() +
  ggtitle("ROC curve Xgboost") +
  theme(plot.title = element_text(hjust = 0.5))

```

\pagebreak

Accuracy and Kappa
These are the default metrics used to evaluate algorithms on binary and multi-class classification datasets in caret.

Accuracy is the percentage of correctly classifies instances out of all instances. It is more useful on a binary classification than multi-class classification problems because it can be less clear exactly how the accuracy breaks down across those classes (e.g. you need to go deeper with a confusion matrix). Learn more about Accuracy here.

Kappa or Cohen’s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0). Learn more about Kappa here.

Use precision and recall to focus on small positive class — When the positive class is smaller and the ability to detect correctly positive samples is our main focus (correct detection of negatives examples is less important to the problem) we should use precision and recall.

Precision and recall are two extremely important model evaluation metrics. While precision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by your algorithm. Unfortunately, it is not possible to maximize both these metrics at the same time, as one comes at the cost of another. For simplicity, there is another metric available, called F-1 score, which is a harmonic mean of precision and recall. For problems where both precision and recall are important, one can select a model which maximizes this F-1 score. For other problems, a trade-off is needed, and a decision has to be made whether to maximize precision, or recall.

\pagebreak

```{r model_comp, cache= FALSE, fig.cap="\\label{fig:model_comp}Comparing models with Accuracy and Kappa"}


# Look at the performance

models <- list(glm = model_glm,
               rf = model_rf,
               xgb = model_xgb)

resampling <- resamples(models)

bwplot(resampling, main = "Comparing of the models")

comp_table <- summary(resampling) 

  
caption_at_bottom(stargazer(comp_table$statistics$Accuracy, header = FALSE, title = "Accuracy" ))
caption_at_bottom(stargazer(comp_table$statistics$Kappa, header = FALSE, title = "Kappa"))


```

\pagebreak

# Assignment 5

## 5.1. RWA interpretation

Risk-weighted asset (RWA) is defined as an aggregated measure, which evaluates amount of regulatory capital that must be reserved by banks or financial institutions to maintain their solvency. RWA is used to: 

(i) provide a common measure for a bank’s risks; 
(ii) ensure that capital allocated to assets is commensurate with the risks; and
(iii) potentially highlight where destabilizing asset class bubbles are arising.

To be clearer, RWA shows how many percentage the bank can absorb losses by comparing the amount of a bank’s capital with the amount of its assets. For example, if its capital is 10% of its assets, then it can lose 10% of its assets without becoming insolvent.

There are rules for risk weighting. Basel I framework, which was set by the global banking regulator, BCBS, used a comparatively simple system. Each class of asset was assigned a fixed risk weight. In 2004, the framework was updated with Basel II and brought a significant innovation with IRB approach. In this framework, a different classification of assets are used to weigh the asset. For particular, probability of default (PD) is the key input for RWA. Total risk-weighted assets are determined by multiplying the capital requirements for market risk and operational risk by 12.5.
When getting their own RWA values, banks have a broad analysis whether they are still capable of taking on more risks. 

## 5.2. RWA calculation

Understanding the definition of RWA, we now continue to apply it for our dataset. To calculate the RWA, the prediction models in the assignment are used for the results of PD for each customer.

```{r}

#define values of some constants

LGD = 0.45
EAD = 100
M = 2.5
S = 50


#calculate RWA according to RWA formula and glm 

glm <- data.frame(R = 0.12*((1-exp(-50*glm_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*glm_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))


glm$b = (0.11852 - 0.05478*ln(glm_predictions$Yes))^2

glm$K = (LGD*pnorm(((1-glm$R)^(-0.5))*qnorm(glm_predictions$Yes) + 
          ((glm$R/(1-glm$R))^0.5)*qnorm(0.999)) - glm_predictions$Yes*LGD)*
  ((1-1.5*glm$b)^(-1))*(1+((M-2.5)*glm$b))

glm$RWA = glm$K*12.5*EAD

#calculate RWA according to RWA formula and random forest

rf <- data.frame(R = 0.12*((1-exp(-50*rf_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*rf_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))

rf$b = (0.11852 - 0.05478*ln(rf_predictions$Yes))^2

rf$K = (LGD*pnorm(((1-rf$R)^(-0.5))*qnorm(rf_predictions$Yes) + 
          ((rf$R/(1-rf$R))^0.5)*qnorm(0.999)) - rf_predictions$Yes*LGD)*
  ((1-1.5*rf$b)^(-1))*(1+((M-2.5)*rf$b))

rf$RWA = rf$K*12.5*EAD

#calculate RWA according to RWA formula and xgboost

xgb <- data.frame(R = 0.12*((1-exp(-50*xgb_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*xgb_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))

xgb$b = (0.11852 - 0.05478*ln(xgb_predictions$Yes))^2

xgb$K = (LGD*pnorm(((1-xgb$R)^(-0.5))*qnorm(xgb_predictions$Yes) + 
          ((xgb$R/(1-xgb$R))^0.5)*qnorm(0.999)) - xgb_predictions$Yes*LGD)*
  ((1-1.5*xgb$b)^(-1))*(1+((M-2.5)*xgb$b))

xgb$RWA = xgb$K*12.5*EAD


#Compare the RWA in each model

RWA_models <- data.frame(RWA_glm = glm$RWA,
               RWA_rf = rf$RWA,
               RWA_xgb = xgb$RWA)


caption_at_bottom(stargazer(RWA_models, header = FALSE, title  = "RWA comparison"))

```

After that, we base on the formula advised as below to get the RWA needed. 

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned} R = 0.12\cdot\frac{1-exp(-50 \cdot PD)}{1-exp(-50)}+0.24 \cdot [1-\frac{1-exp(-50 \cdot PD)}{1-exp(-50)}]-0.04 \cdot \frac{1-(s-5)}{45}\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}MA = (0.11852-0.05478 \cdot ln(PF))^2\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}CR = [LGD \cdot N [(1-R)^{-0.5} \cdot G(PD) + (\frac{R}{1-R})^{0.5}\cdot G(0.999)]-PD \cdot LGD]\cdot (1-1.5 \cdot b)^{-1}\cdot(1+(M-2.5)\cdot b)\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}RWA = K \cdot 12.5 \cdot EAD\end{aligned}
  $}
\]


```{r rwa_plots, cache= FALSE, fig.cap="\\label{fig:rwa_plots}RWA comparison"}

p_glm <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) + 
            geom_line(aes(y = RWA_glm), color = "steelblue") +
            ggtitle("GLM logistic") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")


p_rf <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) +
            geom_line(aes(y = RWA_rf), color="darkgreen") +
            ggtitle("Random Forest") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")
    
            
p_xgb <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) +          
            geom_line(aes(y = RWA_xgb), color ="darkred") +
            ggtitle("xgBoost") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")
     
grid.arrange(p_glm, p_rf, p_xgb, nrow = 3)

```

According to the summary table and the plots for each models, logistic regression model yields lowest RWA while those of random forest are the largest. Interpreted from the construction of the formula above, RWA covariates with PD. The more chances it is that customer will make a default on paying debt, the more assets bank or financial institutions should hold to avoid insolvency. However, the relationship is not proportional, and a high increase in the PD will typically translate into a more moderate increase in RWA. Therefore, it is understandable for logistic regression model to have lowest RWA since PD under that model is also the lowest. 

\pagebreak

# Appendix

```{r impute_pmm, out.width='100%', fig.cap="\\label{fig:impute_pmm}Imputation using PMM"}

### load images 

include_graphics('imputation_pmm.jpeg')

```

```{r impute_mean, out.width='100%', fig.cap="\\label{fig:impute_mean}Imputation using mean"}

### load images 

include_graphics('imputation_mean.jpeg',)

```

```{r tuning_rf, out.width='100%', fig.cap="\\label{fig:tuning_rf}Tuning of Random Forest model"}

### load images 

include_graphics('rf_tuning_plot.jpeg',)

```

```{r tuning_xgb, out.width='100%', fig.cap="\\label{fig:tuning_xgb}Tuning of xgboost model"}

### load images 

include_graphics('xgb_tuning_plot.jpeg',)

```
