
---
title: "Habits"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
---


```{r setup, echo = F}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, results = 'asis', out.height = "\\textheight",  out.width = "\\textwidth")

```


\pagebreak


```{r, cache = FALSE}

library(dplyr)
library(magrittr)
library(tidyr)
library(ggplot2)
library(Hmisc)
library(purrr)
library(caret)
library(ranger) 
library(xgboost)
library(kableExtra) # just to make the output nicer
library(mice)
library(stargazer)
library(xtable)
library(pROC) # think we use yardstick instead, nicer plots
library(knitr)
library(grid)
library(yardstick)
library(SciViews)
library(gridExtra)

options(xtable.comment=FALSE)

theme_set(theme_bw()) 

df <- read.csv("pd_data_v2.csv", sep = ";", header = TRUE)

df_corp <- df

```

# Task 1: Predicion modelling

In this part we will first inspect, clean and impute the data set and the associated variables. We then use stepwise and manual variable selection, as well as test for multicollinearity, to choose the best possible starting point for our prediction model. This is to ensure that when we create and run the model, it is reliable and give predictions that make sense. Finally, we create our models, logistic, random forest and xgboost, and look at how they differ from each other.

We will train and run three different prediction models based on a data set containing financial records of a list of companies. In total, there are 24 variables in the data set and 128070 observations. Prior to training our prediction models, we load and examine the data set.


## Part 1: Inspecting, cleaning and imputing the data set 


### Missing values and outliers
We first wish to look at the distribution of our data, and therefore create a summary statistics, shown in table xx. From this table, we see that many of the observations have a value of +/- 100 000 000’’’’’ (5 more triple zeros). Most likely, these values represent measurement errors or missing variables. They do not make economically sense, but we do not know if the errors occur at random or not. They seem to appear in some of the variables, and not all, but we do not have enough information to make a justified decision about whether they are random. It is therefore not ideal to just remove them (Swalin, 2018), and we choose to replace these values with NAs. 

After completing this step, we find from the box plots (in the appendix) that there are still some extreme outliers remaining. Our assessment is that these values do not represent real observations, as they deviate from typical values for the financial key figures represented in the variables in question. One solution to this is to remove them, but that could bias the dataset. Therefore, applying a threshold of 2,5 percent at each end of the variables' distribution, we replace values exceeding this threshold with NAs. We will later deploy an imputation technique on all the observations replaced with NAs. See further discussions in sections xx.  

Next, we want to look at whether all the variables are classified correctly. We believe the variables unpaid debt collection and paid debt collection are better measured as categorical variables. This is due to their distribution, which includes many extreme values despite our previous cleaning. We therefore encode both these variables as dummies. Adverse audit opinion is also a categorical variable, with several classes. As these do not make sense, and could result in a poorer model, we also dummify these into 0, for those having no adverse audit opinions, and 1, for those that do have adverse audit opinions. 

A final step is to make sure all the variables are categorized logically. We find that there are some variables which should be recategorized. Equity, total asset value, and age of company are changed to be numeric variables, whilst default, adverse audit opinion, industry and payment reminder are changed to factors. We now only have numeric and factorial variables. 


.  

```{r, cache= TRUE}


# Summary statistics 

caption_at_bottom <- function(expr) {
  x <- capture.output(expr)
  cap <- grep("\\\\caption", x)
  lab <- grep("\\\\label", x)
  last <- grep("\\\\end\\{table", x)
  cat(
    paste(
      c(x[-last], x[cap], x[lab], x[last])[-c(cap, lab)]
    , collapse = "\n")
  , "\n")
}

caption_at_bottom(stargazer(df_corp, summary.stat = c("min","max"),header = FALSE, title  = "Summary statistics"))

# As described in the text, outliers are replaced with NA's. See chunch no. xx. 

```
### Data imbalance
When looking at our data, we notice that the variable \texit{default}, which will be the dependent variable in our prediction models, is imbalanced. That is, observations of defaulting companies are underrepresented, which will make it difficult to predict defaults. The underrepresentation of defaults is illustrated in figure \ref{fig:imbalance}. We will handle this imbalance when we implement the prediction models in section xx.


```{r hist, cache=FALSE, fig.cap="\\label{fig:hist}Histogram of the distribution"}

# Distribution of defaults and not defaults

df_corp$default <- as.factor(df_corp$default) 

df_corp %>% 
  ggplot(aes(x = default, fill = default)) +
  geom_bar() +
  ggtitle("Distribution of the non-defaulters and defaulters") +
  theme(plot.title = element_text(hjust = 0.5))

```


```{r, cache= TRUE}

# Mia: Should we consider removing some of these lines? 

# Look at the distribution and recategorize the factor classes

# Adverse_audit_opinion



# xtable(table(df_corp$default, df_corp$adverse_audit_opinion))

df_corp$adverse_audit_opinion <- 
  ifelse(df_corp$adverse_audit_opinion == 0, yes = 0, no = 1)

# xtable(table(df_corp$default, df_corp$adverse_audit_opinion)) # If you got any Adverse audit opinons then its show as 1

# Industry

# xtable(table(df_corp$default, df_corp$industry)) # As we don't know anything about the type of industry we let it be

# Payment_reminders

# xtable(table(df_corp$default, df_corp$payment_reminders)) # Ok

```


```{r}

# Change classes

df_corp$adverse_audit_opinion <- as.factor(df_corp$adverse_audit_opinion) 
df_corp$industry <- as.factor(df_corp$industry)
df_corp$payment_reminders <- as.factor(df_corp$payment_reminders) 
df_corp$equity <- as.numeric(df_corp$equity)
df_corp$total_assets <- as.numeric(df_corp$total_assets)
df_corp$revenue <- as.numeric(df_corp$revenue)
df_corp$age_of_company <- as.numeric(df_corp$age_of_company)

```

```{r}

# Replacing outliers with NA


# First: Take a closer look at the distribution of the values we belive are errors

x <- df_corp %>% 
  select_if(is.numeric)

placeholder <- matrix(ncol=ncol(x), nrow=1)
colnames(placeholder) <- names(x)


for(i in 1:ncol(x)){
  placeholder[,i] <- ifelse(x[,i] > 100000000000 , yes = 1 , 
                            no = ifelse(x[,i] < -100000000000, yes = 1, no = 0)) %>% 
    sum()
}



# as.data.frame(placeholder/nrow(df_corp)) %>%
#  gather() %>% 
#  xtable()

# Do we want to mention this fraction in the text?

# Change the obvious errors to NA

df_corp[df_corp < -1000000000000000] <- NA 
df_corp[df_corp >  1000000000000000] <- NA


```


```{r pre_boxplot, cache = TRUE, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:pre_boxplot}Before cleaning"}

# We take a new look after replacing the errors with NA

df_corp %>% 
  select(which(sapply(.,class)=="numeric"),default) %>%   
  gather(metric, value, -default) %>% 
  ggplot(aes(x= default, y=value, fill = default))+
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

# These boxplots can be viewed in the appendix.

```


```{r, cache= TRUE}

################## Dealing with the outliers ######################


# Function for replacing outliers with NA's

remove_outliers_na <- function(x) { # To be used on other variables
  qnt <- as.vector(quantile(x, probs=c(0.025, 0.975), na.rm = TRUE))
  y <- x
  y[x < qnt[1]] <- NA
  y[x > qnt[2]] <- NA
  y
}

# Strategy: Replace observarions exceeding treshold values of 0.025 (lower bound) 0.975 (upper bound) with NA. This is done for all numeric variables except from age_of_company, gross_operating_inc_perc, paid_debt_collection and unpaid_debt collection.

#           paid and unpaid debt collection will be categorized.

df_corp$profit_margin <- remove_outliers_na(df_corp$profit_margin)
df_corp$operating_margin <- remove_outliers_na(df_corp$operating_margin)
df_corp$EBITDA_margin <- remove_outliers_na(df_corp$EBITDA_margin)
df_corp$interest_coverage_ratio <- remove_outliers_na(df_corp$interest_coverage_ratio)
df_corp$cost_of_debt <- remove_outliers_na(df_corp$cost_of_debt)
df_corp$interest_bearing_debt <- remove_outliers_na(df_corp$interest_bearing_debt)
df_corp$revenue_stability <- remove_outliers_na(df_corp$revenue_stability)
df_corp$equity_ratio <- remove_outliers_na(df_corp$equity_ratio)
df_corp$equity_ratio_stability <- remove_outliers_na(df_corp$equity_ratio_stability)
df_corp$liquidity_ratio_1 <- remove_outliers_na(df_corp$liquidity_ratio_1)
df_corp$liquidity_ratio_2 <- remove_outliers_na(df_corp$liquidity_ratio_2)
df_corp$liquidity_ratio_3 <- remove_outliers_na(df_corp$liquidity_ratio_3)
df_corp$equity <- remove_outliers_na(df_corp$equity)
df_corp$total_assets <- remove_outliers_na(df_corp$total_assets)
df_corp$revenue <- remove_outliers_na(df_corp$revenue)
df_corp$amount_unpaid_debt <- remove_outliers_na(df_corp$amount_unpaid_debt)

# Defining 0 and 1 as "yes" and "no", respectively: 

df_corp$default <- recode_factor(
  df_corp$default,
  `0` = "No",
  `1` = "Yes")

# We remove "equity ratio stability", as it is appear"
df_corp <- df_corp %>% 
  select(-equity_ratio_stability) 

#Mia: Meybe we can do this in another chunk?

```
### Imputation
After cleaning the data set, we are left with xx missing values, that we have to deal with. If not, this could impact the quality of our model (Badr, 2019). It is, first of all, important to note that a good way to deal with missing values does not exist (Swalin, 2018). The easiest way to fix all the missing variables would be to just delete them, if you have enough data. However, if the data is not missing at random, deleting the missing values would make the data biased, or remove important information (Badr, 2019). It is also in general better to impute the missing variables than to just delete them. As the missing values account for a substantial fraction of the total number of observations in the data set, we choose to impute these values. 

A common way to impute is by replacing the missing values with the mean or median of the variable in question. Unfortunately, this would change the mode and thus the data set quite a bit. We also find that this method is better suited when you have continuous variables, rather than categorical (Swalin, 2018). A better solution for us would be to either use logistic regression, multiple imputation or make the missing data as level. We have chosen to use multiple imputation by chained equation (MICE), more specifically predictive Mean Matching (PMM). The imputed variables created by PMM appear closer to real values than those given by methods based on linear regression and normal distribution (Allison, 2015). One advantage with this technique is that it be used on classified variables. Moreover, it does not suggest values that are out of range for the variable that is being imputed (Allison, 2015). A comparison of the results of a PMM imputation and an imputation using mean values is added to the appendix. We believe that though there is no perfect way to solve the issue of missing values, this is a good alternative.

Some models, such as XGboost, are better suited to handle missing data, than the logistic one (Badr, 2019). We have, however, decided to use the imputed data for all three models, as the errors were so large, and the following NAs so many. 


```{r,fig.asp=1, fig.width = 14, eval=FALSE}

# Imputing NAs

temp_imputed1 <- mice(df_corp, m=2,maxit=3,meth='mean',seed=500)

temp_imputed2 <- mice(df_corp, m=2,maxit=3,meth='pmm',seed=500)

saveRDS(temp_imputed1, file = "imputeMEAN.Rdata")

saveRDS(temp_imputed2, file = "impute.Rdata") 

densityplot(temp_imputed1, drop.unused.levels = TRUE) # Save plots again

densityplot(temp_imputed2, drop.unused.levels = TRUE) # Save plots again
?densityplot

# Running the imputations and generating density plots of the imputed values is higly time consuming. Therefore, we have saved these files and will upload them in chunk xx. 

# See appendix for plots. 
```

```{r, include = FALSE}

# Uploading imputed values

temp_imputed1 <- readRDS(file = "imputeMEAN.Rdata")
temp_imputed2 <- readRDS(file = "impute.Rdata")

# We complete the impution and check the result

complete_imputed <- complete(temp_imputed2, 1)

Orginal <- dim(df_corp)
Imputed <- dim(complete_imputed)

xtable(rbind(Orginal, Imputed))

df_cleaned <- complete_imputed

```


```{r, include = FALSE}

# Generating dummies for paid and unpaid debt collection

df_cleaned$unpaid_debt_collection <- ifelse(df_cleaned$unpaid_debt_collection > 0, yes = 1, no = 0) # 1 = Companies that have unpaid debt collection

df_cleaned$paid_debt_collection <- ifelse(df_cleaned$paid_debt_collection > 0, yes = 1, no = 0) # 1 = Companies that have previously had debt collection

df_cleaned$paid_debt_collection <- as.factor(df_cleaned$paid_debt_collection)
df_cleaned$unpaid_debt_collection <- as.factor(df_cleaned$unpaid_debt_collection)

#table1 <- table(df_cleaned$default, df_cleaned$unpaid_debt_collection)
#table2 <- table(df_cleaned$default, df_cleaned$paid_debt_collection)

#xtable(table1)
#xtable(table2)

```

After cleaning, imputing and reclassifying our variables, we are left with data that is well suited for prediction modelling. Figure xx shows the distribution of the variables in our new dataset that will be used moving forward. We see from the plot that our data looks much better now, than before we cleansed, imputed and reclassified it.



```{r Density_post_cleaning, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:Density_post_cleaning}Density plot after cleaning"}

# Density plot after cleaning


df_cleaned %>% 
  select_if(is.numeric) %>% 
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric))+
  geom_density(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

```

```{r Boxplot_post_cleaning, fig.asp=1, fig.width = 14, fig.cap="\\label{fig:Boxplot_post_cleaning}Boxplot after cleaning", include=FALSE}

# Boxplot after cleaning

df_cleaned %>% 
  select(which(sapply(.,class)=="numeric"),default) %>%   
  gather(metric, value, -default) %>% 
  ggplot(aes(x= default, y=value, fill = default))+
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ metric, scales = "free")

#This can be viewed in the appendix.

```

##Prediction modelling

In this section we will train and rund three different prediction models; a Logistic Regression Model, a Random Forest and a Xgboost Model. 

### Cross Validation/Bias Variance tradeoff

Working with such a large data set as we have in this assignment allows for training models that fits the underlying data very well, which risks overfitting the models. Overfitting is undesirable because the fit obtained will not yield accurate predictions on new observations (Source: Statistical learning book, cite this properly). At the same time, it is desirable to reduce the model’s bias, i.e. the inability to capture the true relationship between the variables, which naturally involves increasing the model’s flexibility and thereby risking overfitting the model. This issue is referred to as the bias-variance tradeoff (statistical learning book, cite properly). 

A common approach to control for overfitting is the validation method. Here, one stratifies the data into a training set and a test set, and then train and test the model on the two sets, respectively. This will however introduce some bias, as the model trained with the training set not necessarily is comparable to the model run on the remaining data. Bias is reduced when a larger portion of the data can be used for training. The method that exploits the largest proportion of the data set when training Leave One Out Cross Validation (LOOCV). Here, the test data consists of only one observation, while the remaining observations are used for training. This is done repeatedly for different splits of training and test data. This approach naturally introduces more variance than working with relatively less observations in the training data. 

In general, the key is to train a model where a sufficiently large portion of the data is used for training to increase flexibility and reduce bias, while at the same time, we want we want enough data points for testing in order to keep the variance at a low level (Joglekar, 2015)

A good approach to do this is to use k-fold Cross Validation when we train our models. This approach involves randomly dividing the set of observations into k folds. The first fold is treated as a validation set, and the model is trained on the remaining k-1 folds. Then another fold is treated as a validation set whereas the remaining folds are used for training. This is repeated for all folds. The choice of k, i.e. the number of folds, matters for the bias-variance tradeoff. A large value implies an approach similar to the LOOVC and will give high variance in the testing procedure. A small value of k implies getting closer to the validation set approach. 

We have chosen to do a 10 fold cross validation with 10 repetitions. Cross validation is used for all of the three different prediction models. Choosing 10 folds have been shown empirically to yield results that suffer neither from excessively high bias nor from very high variance (Source: Statistical book)


### Dealing with data imbalance
As mentioned in section xx, the actual defaults are underrepresented in our data set, which will make it difficult to accurately predict defaults in our models. This is called imbalanced data, meaning our classes are not equally represented (Brownlee, 2015). An issue here, if we do not handle the imbalance, is that our model will have a high accuracy, but be completely useless, as it is only representing the imbalance already present. We can approach this kind of data two ways; either we can assume it is not an accurate representation of reality, and set out to gather more data, meaning the reality is balanced. Or, we can assume that reality actually is imbalanced and thus, the data will also be so (Rocca, 2019). As we are dealing with companies defaulting on their loans, we find that the default/non-default ratio seems accurate. We therefore assume that our data is a true representation of reality, and need to handle the imbalance accordingly. 

A common way to deal with this is to use a resampling technique (Towards Data Science, 2018). One could approach this by \textit{undersampling} the data, i.e. remove observations form of firms that did not default. Alternatively, one could \textit{oversampling} the data - that is, scale up the number of defaulting companies. Each approach has its drawbacks. Undersampling can lead to loss of information, making it difficult for the model to predict significant results. One problem with oversampling is that it creates new observations by duplicating data that already exists in the dataset. This way, even though a higher proportion of the dataset are defaulters, they do not contain any new information that increases the model’s predictive power.

Luckily, modified versions have been developed in order to mitigate some of these issues. We have chosen to deploy the Synthetic Minority Oversampling Technique (SMOTE) (Frei, 2019). SMOTE’s main advantage compared to a simple oversampling is that it creates synthetic observations instead of reusing observations that already exist in the dataset. This way, it is less likely to overfit the classifier. In all our models, we have incorporated SMOTE in the 10 fold cross validation procedure. 

Changing the performance metric from accuracy, could also help with our imbalanced data set (Brownlee, 2015), and we have chosen to instead use sensitivity, specificity, a confusion matrix, ROC curves, precision, F1 and several other metrics. These will be discussed later.


### Removing non-relative values

Our data have several variables where the values are non-relative, meaning they look at absolute differences rather than ratios. We tried to calculate different variables using these, but this yielded odd values and numbers, that did not make sense economically. Thus, we remove these variables, namely \textit{equity}, \textit{total assets}, \textit{revenue}, \textit{gross operating income} and \textit{amount unpaid debt}.

After removing the non-relative variables, we the data set into a train and test, with 70% of our observations as the training set, and the remaining 30 % as a test set.


### Logistic Regression

#### Variable Selection

Prior to training our models, we need to consider potential correlation between the variables in our data set. When independent variables correlate with each other, it becomes difficult to interpret a variable’s influence on the outcome variable as a change that occurs “all else equal” (Woolridge, p. 97, YEAR). One way to control for multicollinearity is by computing the Variance Inflation Factor (VIF). The VIF of a variable is the ratio of the variance of the variable’s coefficient when fitting the full model divided by the variance of its coefficient when it is fit to the model on its own. When assessing multicollinearity, it is common to set a threshold for the VIF somewhere between 5 and 10. (Source: statistical learning book). 

To control for multicollinearity we run a logistic regression and then calculate the VIF for the different independent variables in our data set. As seen in table xx, the variables  \textit{operating margin} and \textit{EBITDA margin} have VIF values of xxx and xxx, respectively. When running the model without \textit{operating margin} and recalculating the VIF, none of the VIF exceed the value 5. When we train our logistic regression model in section xx,   \textit{operating margin} will be left out.

We conduct stepwise selection to evaluate the model and remove variables that have little predictive power. Here, we seek to minimize AIC (NCSS Statistical Software, n.d.).
The lowest AIC is obtained when we exclude the following variables: \textit{Paid debt collection}, \textit{profit margin} and \textit{liquidity ratio 1}.

As a final control, we calculate the VIF again, but this time without detecting any potential multicollinearity issues. We feel confident with going further with training our model. 


```{r, include = FALSE, eval= FALSE}

# Last thing we do before we split the data and start modelling is checking for mulitcollinearity.
# We use vic function in the car package.

library(car)

mc_vif_1 <- vif(glm(default ~ ., data = df_reduced, family = "binomial"))

mc_vif_1

# We see that for the glm we should remove operating margin

mc_vif_2 <- vif(glm(default ~ .-operating_margin, data = df_reduced, family = "binomial"))

mc_vif_2 # By checking again all looks okay

# Stepwise

# stepwise <- step(glm(default ~ .-operating_margin, data = df_reduced, family = "binomial"), direction="both")

# Removed after stepwise

mc_vif_3 <- vif(glm(default ~ .-operating_margin - paid_debt_collection - liquidity_ratio_1 - profit_margin, data = df_reduced, family = "binomial"))

mc_vif_3
```


```{r}

# For all models we will remove the equity, total_assets, revenue and amount_unpaid_dept because we want to work with relative values.

df_reduced <- df_cleaned %>% 
  select(-equity, -total_assets, -revenue, -amount_unpaid_debt)

``` 



```{r}

# Split data into train/test

set.seed(1)

index <- createDataPartition(df_reduced$default, p = 0.7, list = FALSE)
train_data <- df_reduced[index, ]
test_data <- df_reduced[-index, ]

# Check

# nrow(train_data) + nrow(test_data) == nrow(df_reduced)

```

```{r, include = FALSE}

`Train defaults`<- summary(train_data$default)[2]/nrow(train_data)

`Test defaults`<- summary(test_data$default)[2]/nrow(test_data)

xtable(rbind(`Train defaults`,`Test defaults`))

```

 
```{r}

############# Setting the control to be used fro all three models  ###################

ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats =10, 
                     verboseIter = FALSE,
                     sampling = "smote",
                     classProbs=TRUE)

```

### Logistic Regression Model

#### Assumptions for logistic regression

There are some assumptions of logistic regression that have to be satisfied, to interpret the model and have reliable predictions (Rivenæs, 2019). When dealing with binary logistic regression, the dependent variable must be binary (Statistics Solutions, n.d.), which our is, fulfilling the assumption. You also need at least one independent variable, either continuous or categorical (Lærd Statistics, n.d.), which our model also has. Both the observations and the variables should be independent (Statistics Solutions, n.d.). We have ensured that the variables are independent, meaning little or no multicollinearity, by excluding those correlating in the variable selection, using AIC as our measurement. It is possible that some of the observations are related, such as if there are subsidiary and parent companies or cooperating companies. However, with the large amount of observations in our data set, we assume this will have little effect, fulfilling this assumption as well.  Next, logistic regression assumes there is a linear relationship between the independent variables and the log odds. We assume this is also fulfilled, partially because there is no multicollinearity present. Finally, we have a large sample size.


#### Training the logistic regression model

We run a logistic regression model using \textit{default} as the dependent variable. 


Summary statistics from the logistic regression model are presented in table xxxx. As shown in the table, most variables are statistically significant. The exceptions are \text{liquidity ratio 2} and \textit{industry} 1 -3 and 4-11.

The model’s confusion matrix is shown in table xx and an overview of the variables’ importance is provided in figure xx. We will discuss the models’ performance in section xx. 

\pagebreak


```{r}

# After we have run the model, we see that industry and gross income percentage have some insignificant values.
# We try to remove gross and ndustry one by one and together. We got lowest AIC by only removing
# gross_operating_inc_perc. 

set.seed(1)

model_glm <- train(default ~ .- operating_margin 
                              - paid_debt_collection 
                              - liquidity_ratio_1 
                              - profit_margin
                              - gross_operating_inc_perc,
                   data = train_data,
                   method = "glm",
                   trControl = ctrl)

# To mangage to run the latter codes due to removal of variables in the regression.



```



```{r var_imp_GLM, fig.cap="\\label{fig:var_imp_GLM}Variable importance"}

summary_glm <- xtable(summary(model_glm), caption = "Summary glm model")

print(summary_glm, scalebox=1, caption.placement = "bottom", label = "tab:sumtable")

plot(varImp(model_glm), main = "Variables Importance GLM")



glm_pred <- data.frame(actual = test_data$default,
                       predict(model_glm, newdata = test_data, type = "prob"))

glm_pred$predict <- ifelse(glm_pred$Yes > 0.5, "Yes", "No")
glm_pred$predict <- as.factor(glm_pred$predict)

cm_glm <-confusionMatrix(glm_pred$predict, test_data$default, positive="Yes")


confusion_glm <- xtable(as.matrix(cm_glm$table), caption = "Confusion matrix GLM")

print(confusion_glm, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_glm")



perf_glm <- as.matrix(cm_glm$byClass)
colnames(perf_glm) <- c("Values")

performance_glm <- xtable(perf_glm, caption = "Performance indicators GLM")

print(performance_glm, scalebox = 1, caption.placement = "bottom", label = "tab:performance_glm")

```


```{r ROC_glm, fig.cap="\\label{fig:ROC_glm}ROC plot for glm logistic"}

# ROC curve glm

glm_predictions <- predict(model_glm, test_data, type="prob")

test_glm <- test_data %>%
  select(default) %>% 
  mutate(glm_prob_predictions = glm_predictions$Yes)

glm_auc <- test_glm %>% 
              roc_auc(default, glm_prob_predictions)

glm_auc <- paste(round(glm_auc$.estimate, 4), sep = "")

test_glm %>%
  roc_curve(default, glm_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = glm_auc) +
  coord_equal() +
  ggtitle("ROC curve GLM (Logistic)") +
  theme(plot.title = element_text(hjust = 0.5))
 

```
### Tree models
There are no assumptions for Random Forest (Richmond, 2016). Concerning XGBoost, whilst we could not explicitly find that there are no assumptions, we found that this is a powerful tree algorithm (Brownlee, 2015). We also find that tree models circumvent the assumptions for regression models (Merkle & Phelps, 2008), and thus assume there are no assumptions for this model as well. 

#### Tuning the tree models

In both the Random Forest and the XGboost model, the parameters are tuned in order to optimize the performance of the models.

In the Random Forest we specify vectors containing potential values for the number of trees and the number of variables that are available for each three. When working with the hyperparameters in the Xgboost model, we pick out some variables we want to tune, as tuning too many parameters is highly time consuming. The ones we choose to tune are  the learning rate (eta), the number of trees to grow (nrounds), the maximum tree depth and gamma. The results of the hyperparameter tuning is added to the appendix. 


#### Random Forest
We train the Random Forest model and print its associated confusion matrix, ROC curve and variable importance. The results are presented in table xx and figures xx and xx, respectively. 






```{r, eval= FALSE, fig.asp=1, fig.width = 14}

############################ Model 2: Random Forest ######################################

tunegrid <- expand.grid(.mtry= 4,
                        .splitrule = "gini",
                        .min.node.size = 10) # tuning with mtry = 1:5, min.node.size = 10,20

set.seed(1)

model_rf <- caret::train(default ~ .,
                         data = train_data,
                         method = "ranger",
                         trControl = ctrl,
                         tuneGrid = tunegrid,
                         num.trees = 100,
                         importance = "permutation")

# saveRDS(model_rf, file = "rf.Rdata")

```

\pagebreak


```{r var_imp_rf, fig.cap="\\label{fig:var_imp_rf}Variables importance Random Forest"}

model_rf <- readRDS("rf.Rdata")

rf_pred <- data.frame(actual = test_data$default,
                       predict(model_rf, newdata = test_data, type = "prob"))

rf_pred$predict <- ifelse(rf_pred$Yes > 0.5, "Yes", "No")
rf_pred$predict <- as.factor(rf_pred$predict)

plot(varImp(model_rf),main = "Variables Importance Random Forest")

cm_rf <- confusionMatrix(rf_pred$predict, test_data$default)

confusion_rf <- xtable(as.matrix(cm_rf$table), caption = "Confusion matrix Random Forest")

print(confusion_rf, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_rf")

perf_rf <- as.matrix(cm_rf$byClass)
colnames(perf_rf) <- c("Values")

performance_rf <- xtable(perf_rf, caption = "Performance indicators Random Forest")

print(performance_rf, scalebox = 1, caption.placement= "bottom", label = "tab:performance_rf")

```

```{r ROC_rf, fig.cap="\\label{fig:ROC_rf}ROC plot for Random Forest"}

# ROC curve rf

rf_predictions <- predict(model_rf, test_data, type = "prob")

test_rf <- test_data %>%
  select(default) %>% 
  mutate(rf_prob_predictions = rf_predictions$Yes)

rf_auc <- test_rf %>% 
              roc_auc(default, rf_prob_predictions)

rf_auc <- paste(round(rf_auc$.estimate, 4), sep = "")

test_rf %>%
  roc_curve(default, rf_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = rf_auc) +
  coord_equal() +
  ggtitle("ROC curve Random Forest (Ranger)") +
  theme(plot.title = element_text(hjust = 0.5))

```

\pagebreak


#### XGBoost
We train the XGBoost model and print its associated confusion matrix, ROC curve and variable importance. The results are presented in table xx and figures xx and xx, respectively. 


```{r, eval=FALSE, fig.asp=1, fig.width = 14, eval = FALSE}
#Model 3: Xgboost

xgb_grid <- expand.grid(nrounds = 300,
                        max_depth = 7, 
                        min_child_weight = 1,
                        subsample = 1,
                        gamma = .1,
                        colsample_bytree = 0.8,
                        eta = .1)

set.seed(1)

model_xgb <- caret::train(default ~ .,
                          data = train_data,
                          method = "xgbTree",
                          tuneGrid =xgb_grid,
                          trControl = ctrl)

# Check preProcess - is this necessary, is it right, or is it wrong??

# saveRDS(model_xgb, file = "xgb.Rdata")

```

```{r var_imp_xgb, Cache = FALSE, fig.cap="\\label{fig:var_imp_xgb}Variable importance Xgboost"}

model_xgb <- readRDS("xgb.Rdata")

plot(varImp(model_xgb),main = "Variables Importance Xgboost")

xgb_pred <- data.frame(actual = test_data$default,
                      predict(model_xgb, newdata = test_data, type = "prob"))

xgb_pred$predict <- ifelse(xgb_pred$Yes > 0.5, "Yes", "No")
xgb_pred$predict <- as.factor(xgb_pred$predict)

cm_xgb <- confusionMatrix(xgb_pred$predict, xgb_pred$actual)

confusion_xgb <- xtable(as.matrix(cm_xgb$table), caption = "Confusion matrix xgboost")

print(confusion_xgb, scalebox = 1, caption.placement = "bottom", label = "tab:confusion_xgb")

perf_xgb <- as.matrix(cm_xgb$byClass)
colnames(perf_xgb) <- c("Values")

performance_xgb <- xtable(perf_xgb, caption = "Performance indicators xgboost")

print(performance_xgb, scalebox = 1, caption.placement= "bottom", label = "tab:performance_xgb")

```

```{r ROC_xgb, Cache = FALSE, fig.cap="\\label{fig:ROC_xgb}ROC plot for Xgboost"}

# ROC curve xgb

xgb_predictions <- predict(model_xgb, test_data, type = "prob")

test_xgb <- test_data %>%
  select(default) %>% 
  mutate(xgb_prob_predictions = xgb_predictions$Yes)

xgb_auc <- test_xgb %>% 
              roc_auc(default, xgb_prob_predictions)

xgb_auc <- paste(round(xgb_auc$.estimate, 4), sep = "")

test_xgb %>%
  roc_curve(default, xgb_prob_predictions) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  annotate("text",x = 0.7, y = 0.4, label = xgb_auc) +
  coord_equal() +
  ggtitle("ROC curve Xgboost") +
  theme(plot.title = element_text(hjust = 0.5))

```

\pagebreak


```{r model_comp, cache= FALSE, fig.cap="\\label{fig:model_comp}Comparing models with Accuracy and Kappa"}


# Look at the performance

models <- list(glm = model_glm,
               rf = model_rf,
               xgb = model_xgb)

resampling <- resamples(models)

bwplot(resampling, main = "Comparing of the models")

comp_table <- summary(resampling) 

  
caption_at_bottom(stargazer(comp_table$statistics$Accuracy, header = FALSE, title = "Accuracy" ))
caption_at_bottom(stargazer(comp_table$statistics$Kappa, header = FALSE, title = "Kappa"))


```
\pagebreak

#Assignment 2.2:
## Which other “business” assessments should be carried out in addition to statistical validation tests to assess model performance?
 
After we have developed the three models and validated them through statistical approach, it is necessary to turn to the business angel and make an assessment if the validation results make sense since the purpose of the selected model is to serve for future business prediction. And in probability of default model, we could consider two aspects from business side:
 
###2.1.

The variables used in three models have been listed with their importance (ref. Figure 4, 6, 8) and if we take out the top 5 variables from each of the model, we could compare which model´s variable importance ranking is more complied with business reality.
  
And from the chart above, we found that GLM model has three variables out of top 5 as the payment reminders compare the other two models. And in our model, we have three payment reminders variables because we have turned the original variable into factor, and this give some indication that GLM might be overweighting the variables of payment reminders than the other two models. And since GLM is more sensitive to outliers and multicollinearity issue. So, GLM have five variables less than the other two models as well--- Operating margin, paid debt collection, profit margin, liquidity ratio 1 and gross operating income. And among the excluded values, we find that profit margin is one of the top 5 variables in random forest model. Not including relevant independent variables increases the risk of overlooking causal associations or of confounding them with non-causal ones. It also keeps the unexplained dependent variable variance high and the reliability of our coefficient estimates low (European Social Survey, 2019). So, from the business part, we can see the GLM model underperform random forest and xgboost model.
 
###2.2.

For a more general way of interpretation, except the insufficiency of our dataset. Some factors could be used for a more detailed comparison of different models, for instance, industry. We have different industry categories in the dataset. And from the business reality, some industry might have larger default risk than the others due to their characteristics. And even though we do not have specific industry information with our dataset. But we do discover that the ranking of industry factor from our three models are different. And in the business assessment we could check besides statistic validation to see if the rankings are reasonable with complementary information regarding industry.

###2.3. 

How you would assess model performance if the portfolio had been a low-default portfolio?


####2.3.1. 

Definition of low default portfolios (LDPs) & its disadvantages

Low-default portfolios are those of which default data is limited or non-existent compared to the total numbers of obligators. According to the Bank of England suggestion in Prudential Regulation Authority 2013, if a portfolio that contains fewer than 20 defaults in its data set, it is considered to be a low default portfolio. Moreover, besides the quality of default data, this definition also strongly states the impact of the ratio between default cases and non-default ones.
Low-default portfolio is also described as “imbalance problem” or “imbalanced data”. In this case, risk quantification and validation is in challenge as there is not enough sufficient statistical data. Hence, the credit risk can be substantially underestimated or overestimated. 


####2.3.2. Model performance assessment methodology
It is difficult for LDPs to apply the advanced internal ratings based (IRP) approach like other portfolios. However, on the other hand, the Basel Committee Accord Implementation Group’s Validation Subgroup states that LDPs should still be included in the IRB treatment. To compensate the absence of sufficient data, we could use a set of tools and techniques to facilitate risk assessment. Banks or financial institutions can come up with solutions which are aim to enhance data richness. Those tools are briefly specified as below:
Use external data sources by exchanging, pooling data with other banks or market participants. 

Combine internal portfolio segments with similar risk characteristics
Combine different rating categories then analyse PD of such combination.
Use the upper bound of the PD as an input to formula for RWA if individual PD might be too unreliable.Derive PD estimates under different horizons.
Use lowest non-default rating as a proxy for default if credit support results in low default rate.

Finally, to assess the model performance, we need to select the right evaluation metrics as it will not provide any valuable information. For instance, other alternative evaluation metrics can be applied such as:
AUC (area under the curve) helps to compare ROC curves amongst all models. ROC curve give a vision of  relation between true-positive rate and false positive rate. Therefore, banks or financial institutions can use it to decide the best threshold value. Hence, the model having larger AUC classifies the patients better. 
Precision is true-positive predictive rate while specificity is true-negative rate. This metric is recommended to analyse how many selected instances are relevant to one another.
Recall/sensitivity is positive predictive rate. They are used if false positives is far better than false negatives. 
If you have uneven class distribution, F1 score is a harmonic mean of precision and recall.

\pagebreak

# Assignment 3


\pagebreak

# Assignment 4



\pagebreak

# Assignment 5

## 5.1. RWA interpretation

Risk-weighted asset (RWA) is defined as an aggregated measure, which evaluates amount of regulatory capital that must be reserved by banks or financial institutions to maintain their solvency. RWA is used to: 

(i) provide a common measure for a bank’s risks; 
(ii) ensure that capital allocated to assets is commensurate with the risks; and
(iii) potentially highlight where destabilizing asset class bubbles are arising.

To be clearer, RWA shows how many percentage the bank can absorb losses by comparing the amount of a bank’s capital with the amount of its assets. For example, if its capital is 10% of its assets, then it can lose 10% of its assets without becoming insolvent.

There are rules for risk weighting. Basel I framework, which was set by the global banking regulator, BCBS, used a comparatively simple system. Each class of asset was assigned a fixed risk weight. In 2004, the framework was updated with Basel II and brought a significant innovation with IRB approach. In this framework, a different classification of assets are used to weigh the asset. For particular, probability of default (PD) is the key input for RWA. Total risk-weighted assets are determined by multiplying the capital requirements for market risk and operational risk by 12.5.
When getting their own RWA values, banks have a broad analysis whether they are still capable of taking on more risks. 

## 5.2. RWA calculation

Understanding the definition of RWA, we now continue to apply it for our dataset. To calculate the RWA, the prediction models in the assignment are used for the results of PD for each customer.

```{r}

#define values of some constants

LGD = 0.45
EAD = 100
M = 2.5
S = 50


#calculate RWA according to RWA formula and glm 

glm <- data.frame(R = 0.12*((1-exp(-50*glm_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*glm_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))


glm$b = (0.11852 - 0.05478*ln(glm_predictions$Yes))^2

glm$K = (LGD*pnorm(((1-glm$R)^(-0.5))*qnorm(glm_predictions$Yes) + 
          ((glm$R/(1-glm$R))^0.5)*qnorm(0.999)) - glm_predictions$Yes*LGD)*
  ((1-1.5*glm$b)^(-1))*(1+((M-2.5)*glm$b))

glm$RWA = glm$K*12.5*EAD

#calculate RWA according to RWA formula and random forest

rf <- data.frame(R = 0.12*((1-exp(-50*rf_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*rf_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))

rf$b = (0.11852 - 0.05478*ln(rf_predictions$Yes))^2

rf$K = (LGD*pnorm(((1-rf$R)^(-0.5))*qnorm(rf_predictions$Yes) + 
          ((rf$R/(1-rf$R))^0.5)*qnorm(0.999)) - rf_predictions$Yes*LGD)*
  ((1-1.5*rf$b)^(-1))*(1+((M-2.5)*rf$b))

rf$RWA = rf$K*12.5*EAD

#calculate RWA according to RWA formula and xgboost

xgb <- data.frame(R = 0.12*((1-exp(-50*xgb_predictions$Yes))/(1-exp(-50))) +
                   0.24*(1-(1-exp(-50*xgb_predictions$Yes))/(1-exp(-50))) - 0.04 * ((1-(S-5))/45))

xgb$b = (0.11852 - 0.05478*ln(xgb_predictions$Yes))^2

xgb$K = (LGD*pnorm(((1-xgb$R)^(-0.5))*qnorm(xgb_predictions$Yes) + 
          ((xgb$R/(1-xgb$R))^0.5)*qnorm(0.999)) - xgb_predictions$Yes*LGD)*
  ((1-1.5*xgb$b)^(-1))*(1+((M-2.5)*xgb$b))

xgb$RWA = xgb$K*12.5*EAD


#Compare the RWA in each model

RWA_models <- data.frame(RWA_glm = glm$RWA,
               RWA_rf = rf$RWA,
               RWA_xgb = xgb$RWA)


caption_at_bottom(stargazer(RWA_models, header = FALSE, title  = "RWA comparison"))

```

After that, we base on the formula advised as below to get the RWA needed. 

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned} R = 0.12\cdot\frac{1-exp(-50 \cdot PD)}{1-exp(-50)}+0.24 \cdot [1-\frac{1-exp(-50 \cdot PD)}{1-exp(-50)}]-0.04 \cdot \frac{1-(s-5)}{45}\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}MA = (0.11852-0.05478 \cdot ln(PF))^2\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}CR = [LGD \cdot N [(1-R)^{-0.5} \cdot G(PD) + (\frac{R}{1-R})^{0.5}\cdot G(0.999)]-PD \cdot LGD]\cdot (1-1.5 \cdot b)^{-1}\cdot(1+(M-2.5)\cdot b)\end{aligned}
  $}
\]

\[
  \makebox[\linewidth]{$\displaystyle
    \begin{aligned}RWA = K \cdot 12.5 \cdot EAD\end{aligned}
  $}
\]


```{r rwa_plots, cache= FALSE, fig.cap="\\label{fig:rwa_plots}RWA comparison"}

p_glm <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) + 
            geom_line(aes(y = RWA_glm), color = "steelblue") +
            ggtitle("GLM logistic") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")


p_rf <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) +
            geom_line(aes(y = RWA_rf), color="darkgreen") +
            ggtitle("Random Forest") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")
    
            
p_xgb <- ggplot(RWA_models, aes(x= 1:nrow(RWA_models))) +          
            geom_line(aes(y = RWA_xgb), color ="darkred") +
            ggtitle("xgBoost") +
            theme(plot.title = element_text(hjust = 0.5)) +
            ylab("RWA") +
            xlab("")
     
grid.arrange(p_glm, p_rf, p_xgb, nrow = 3)

```

According to the summary table and the plots for each models, logistic regression model yields lowest RWA while those of random forest are the largest. Interpreted from the construction of the formula above, RWA covariates with PD. The more chances it is that customer will make a default on paying debt, the more assets bank or financial institutions should hold to avoid insolvency. However, the relationship is not proportional, and a high increase in the PD will typically translate into a more moderate increase in RWA. Therefore, it is understandable for logistic regression model to have lowest RWA since PD under that model is also the lowest. 

\pagebreak

# Appendix

```{r impute_pmm, out.width='100%', fig.cap="\\label{fig:impute_pmm}Imputation using PMM"}

### load images 

include_graphics('imputation_pmm.jpeg')

```

```{r impute_mean, out.width='100%', fig.cap="\\label{fig:impute_mean}Imputation using mean"}

### load images 

include_graphics('imputation_mean.jpeg',)

```

```{r tuning_rf, out.width='100%', fig.cap="\\label{fig:tuning_rf}Tuning of Random Forest model"}

### load images 

include_graphics('rf_tuning_plot.jpeg',)

```

```{r tuning_xgb, out.width='100%', fig.cap="\\label{fig:tuning_xgb}Tuning of xgboost model"}

### load images 

include_graphics('xgb_tuning_plot.jpeg',)

```
